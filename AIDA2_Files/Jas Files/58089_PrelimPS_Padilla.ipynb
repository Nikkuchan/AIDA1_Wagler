{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "58089_PrelimPS_Padilla",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikkuchan/CpE-AIML/blob/main/58089_PrelimPS_Padilla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fte_zsqVdp8R"
      },
      "source": [
        "# Topic02a : Prelim Problem Set I\n",
        "\n",
        "Padilla, Jasmine Clare B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpcY5oJ5eFxA"
      },
      "source": [
        "## Case 1\n",
        "Represent the following representations into its vectorized form using LaTeX.\n",
        "> **Problem 1.a. System of Linear Equations**\n",
        "$$\n",
        "\\left\\{\n",
        "    \\begin{array}\\\\\n",
        "        -y+z=\\frac{1}{32}\\\\ \n",
        "        \\frac{1}{2}x -2y=0 \\\\\n",
        "        -x + \\frac{3}{7}z=\\frac{4}{5}\n",
        "    \\end{array}\n",
        "\\right. $$\n",
        "> **Problem 1.b. Linear Combination**\n",
        "$$  \\cos{(\\theta)}\\hat{i} + \\sin{(\\theta)}\\hat{j} - \\csc{(2\\theta)}\\hat{k}$$\n",
        "> **Problem 1.c. Scenario**\n",
        ">\n",
        ">A conference has 200 student attendees, 45 professionals, and has 15 members of the panel. There is a team of 40 people on the organizing committee. Represent the *percent* composition of each *attendee* type of the conference in matrix form.\n",
        "\n",
        "Express your answers in LaTeX in the answer area.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-OKRYSY3qBu"
      },
      "source": [
        "Problem 1.a\n",
        "\n",
        "$$\\begin{bmatrix}0&-1&1\\\\\\frac{1}{2}&-2&0\\\\-1&0&\\frac{3}{7}\\end{bmatrix}=\\begin{bmatrix}\\frac{1}{32}\\\\0\\\\\\frac{4}{5}\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ncANOk4l5s"
      },
      "source": [
        "Problem 1.b\n",
        "\n",
        "$$\\begin{bmatrix}\\cos{(\\theta)}i & \\sin{(\\theta)}j & - \\csc{(2\\theta)}k\\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrQtZQ_D59Jp"
      },
      "source": [
        "Problem 1.c\n",
        "\n",
        "$$Conference = \\left[\\begin{matrix}\\frac{2}{3} & \\frac{3}{20} & \\frac{1}{20} & \\frac{2}{15}\\end{matrix}\\;\\middle|\\;\\begin{matrix}1 \\end{matrix}\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvb1MGs9QNVt"
      },
      "source": [
        "# Case 2\n",
        "> **Problem 2.a: Vector Magnitude**\n",
        "\n",
        ">The magnitude of a vector is usually computed as:\n",
        "$$||v|| = \\sqrt{a_0^2 + a_1^2 + ... +a_n^2}$$\n",
        "Whereas $v$ is any vector and $a_k$ are its elements wherein $k$ is the size of $v$.\n",
        "Re-formulate $||v||$ as a function of an inner product. Further discuss this concept and provide your user-defined function.\n",
        "\n",
        "> **Problem 2.b: Angle Between Vectors**\n",
        "\n",
        "> Inner products can also be related to the Law of Cosines. The property suggests that:\n",
        "$$u\\cdot v = ||u||\\cdot||v||\\cos(\\theta)$$\n",
        "Whereas $u$ and $v$ are vectors that have the same sizes and $\\theta$ is the angle between $u$ and $v$.\n",
        "\n",
        "> Explain the behavior of the dot product when the two vectors are perpendicular and when they are parallel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxsbM3Qn-Egl"
      },
      "source": [
        "##Problem 2.a: Vector Magnitude\n",
        "\n",
        "The magnitude of a vector is determined by its length. The length of a vector must be calculated first before computing its magnitude. Velocity, displacement, force, momentum, and other quantities with direction are examples of vector quantities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyE7fMkrsA3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0feffe81-e72c-415e-c5c1-9760cbd7091b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vect_mag(k):\n",
        "  return np.sqrt(sum(a**2 for a in k))\n",
        "\n",
        "vector = np.array([1, 2, 3, 4, 5])\n",
        "v = vect_mag(vector)\n",
        "\n",
        "print(vector, v, sep = '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n",
            "7.416198487095663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUb76Mjn-fsR"
      },
      "source": [
        "## Problem 2.b: Angle Between Vectors\n",
        "\n",
        "Given that vectors are quantities with direction, there would be something that would indicate that direction. In this case, it's an angle. \n",
        "\n",
        "The dot product when two vectors are perpendicular is null as cos(90) = 0.\n",
        "$$u\\cdot v = ||u||\\cdot||v||\\cos(90)$$\n",
        "$$u\\cdot v = ||u||\\cdot||v||0$$\n",
        "$$u\\cdot v = 0$$\n",
        "\n",
        "The dot product when two vectors are parallel is just the dot product of two vectors as cos(0) = 1.\n",
        "$$u\\cdot v = ||u||\\cdot||v||\\cos(0)$$\n",
        "$$u\\cdot v = ||u||\\cdot||v||1$$\n",
        "$$u\\cdot v = u\\cdot v$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqds07CfsUYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39a557b-355f-4bba-ba95-e4e348214c1c"
      },
      "source": [
        "def angle_vectors(u, v):\n",
        "  inner = np.inner(u, v)\n",
        "  norms = np.linalg.norm(u) * np.linalg.norm(v)\n",
        "  cos = inner / norms\n",
        "  rad = np.arccos(np.clip(cos, -1.0, 1.0))\n",
        "  deg = np.rad2deg(rad)\n",
        "  print(\"Radiant: \", rad)\n",
        "  print(\"Degree: \", deg)\n",
        "  return deg, rad\n",
        "\n",
        "u = np.array([1, 2])\n",
        "v = np.array([-1, -2])\n",
        "print(u,v)\n",
        "\n",
        "angle_vectors(u,v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2] [-1 -2]\n",
            "Radiant:  3.1415926325163688\n",
            "Degree:  179.99999879258172\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(179.99999879258172, 3.1415926325163688)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cH8JpkBj1xS"
      },
      "source": [
        "# Case 3\n",
        "For the final cases analysis we will be looking at series of equations building up a single feed-forward computation of a logistic regression. The case will not require you to learn fully what is logistic regression. \n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "— (x^{(1)})^T— \\\\ \n",
        "— (x^{(2)})^T— \\\\\n",
        "\\vdots \\\\\n",
        "— (x^{(m)})^T— \\\\\n",
        "\\end{bmatrix} \\text{, } \n",
        "Y = \\begin{bmatrix} \n",
        "y^{(1)} \\\\ \n",
        "y^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "y^{(m)} \\\\\n",
        "\\end{bmatrix} \\text{, and } \n",
        "\\theta = \\begin{bmatrix} \n",
        "\\theta^{(1)} \\\\ \n",
        "\\theta^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\theta^{(m)} \\\\\n",
        "\\end{bmatrix} $$\n",
        "The dataset $X$ has $m$ entries with $n$ features while $Y$ is the vector containing the groud truths of a the entries of $X$, and $\\theta$ are the parameters or weights of the vectors. We first compute the vector product of the dataset and the parameters as:\n",
        "$$ z = x^{(i)}\\theta^{(i)} = X\\cdot \\theta\\\\_{\\text{Eq. 3.1}}$$\n",
        "We then solve for the hypothesis of the logistic regression alogrithm as:\n",
        "\n",
        "$$ h_\\theta(x) = g(z)\\\\_{\\text{Eq. 3.2}}$$\n",
        "\n",
        "Where $g$ is an acitvation function that maps the values of the hypothesis vector between a range of 0 and 1. We computed the activation as a sigmoid function:\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}\\\\_{\\text{Eq. 3.3}}$$\n",
        "Finally we compute the loss of the logistic regression algorithm using $J$. Wheras $J(\\theta)$ is a function that computes the logistic loss of the hypothesis with respect to the ground truths $y$. it is then computed as:\n",
        "$$J(\\theta) = \\frac{1}{m} \\sum^m_{i=0}=[-y^{(i)}\\log({h_{\\theta}(x^{(i)})})-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))]\\\\_{\\text{Eq. 3.4}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ8jJV9-qyFy"
      },
      "source": [
        "> **Problem 3.a: Matrix Equivalences**\n",
        "\n",
        "> In Eq. 1, $z$ can also be solved as $X \\cdot \\theta$ which is the vectorized form of $x^{(i)}\\theta^{(i)}$. However, it can also be expressed as $\\theta^T\\cdot X$. Prove the equality of $X \\cdot \\theta$ with $\\theta^T\\cdot X$ in this case.\n",
        "\n",
        "> **Problem 3.b: Matrix Shapes**\n",
        "\n",
        "> Determine the shape of $h_\\theta$ if $X$ has a shape of $(300,5)$.\n",
        "\n",
        "> **Problem 3.c: Vectorization**\n",
        "\n",
        "> Express $J(\\theta)$ into its vectorized form.\n",
        "\n",
        "> **Problem 4.c: Computational Programming (Also Laboratory 2)**\n",
        "\n",
        "> Encode Equations 3.1 to 3.4 as the class `LRegression` wherein:\n",
        "\n",
        "> * `LRegression` should be instantiated with a dataset $X$, a ground truth vector $y$, and a parameter vector $\\theta$. Each parameter should have a data type of `numpy.array`.\n",
        "> * It should further have `methods`reflecting to at least the four (4) aforementioned equations. Each should have a return value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVC1vvwcI8OV"
      },
      "source": [
        "##Problem 3.a: Matrix Equivalence\n",
        "\n",
        "Unlike scalar multiplication, multiplication involving matrices is not commutative. However, commutative property is allowed when getting the dot product between two vectors.\n",
        "\n",
        "\n",
        "It means that $A\\cdot B = B \\cdot A $ can be applied even if one of the vectors is transposed. \n",
        "\n",
        "In problem 3.a, it is required to prove the equality of $X \\cdot \\theta$ with $\\theta^T\\cdot X$ given that $z$ can also be solved as $X \\cdot \\theta$ in equation 3.1\n",
        "$$X = \\begin{bmatrix} \n",
        "— (x^{(1)})^T— \\\\ \n",
        "— (x^{(2)})^T— \\\\\n",
        "\\vdots \\\\\n",
        "— (x^{(m)})^T— \\\\\n",
        "\\end{bmatrix} \\text{, } \n",
        "\\theta = \\begin{bmatrix} \n",
        "\\theta^{(1)} \\\\ \n",
        "\\theta^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\theta^{(m)} \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "By computing the vector product of the dataset and the parameters $$ z = X\\cdot \\theta$$ we get:\n",
        "\n",
        "$$z = \\begin{bmatrix} \n",
        "— (x^{(1)})^T\\theta— \\\\ \n",
        "— (x^{(2)})^T\\theta-\\\\\n",
        "\\vdots \\\\\n",
        "— (x^{(m)})^T\\theta— \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\n",
        "By applying commutative property to $ z = X\\cdot \\theta$ which is the vectorized form of $x^{(i)^T} \\theta^{(i)}$ , we can say that it is equal to \n",
        "\n",
        "$$ z = \\theta^T\\cdot X$$ \n",
        "\n",
        "$$z = \\begin{bmatrix} \n",
        "—\\theta^T(x^{(1)})— \\\\ \n",
        "—\\theta^T(x^{(2)})-\\\\\n",
        "\\vdots \\\\\n",
        "—\\theta^T(x^{(m)})— \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Hence, \n",
        "$$z = \\begin{bmatrix} \n",
        "— (x^{(1)})^T\\theta— \\\\ \n",
        "— (x^{(2)})^T\\theta-\\\\\n",
        "\\vdots \\\\\n",
        "— (x^{(m)})^T\\theta— \\\\\n",
        "\\end{bmatrix} \n",
        "= \\begin{bmatrix} \n",
        "—\\theta^T(x^{(1)})— \\\\ \n",
        "—\\theta^T(x^{(2)})-\\\\\n",
        "\\vdots \\\\\n",
        "—\\theta^T(x^{(m)})— \\\\\n",
        "\\end{bmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm4n53ryKsUa"
      },
      "source": [
        "## Problem 3.b: Matrix Shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSAnLY7LJNSm"
      },
      "source": [
        "## Problem 3.c: Vectorization\n",
        "\n",
        "$$\\begin{bmatrix} \n",
        "\\frac{\\partial{J}}{\\partial{\\theta}_0} \\\\ \n",
        "\\frac{\\partial{J}}{\\partial{\\theta}_1} \\\\\n",
        "\\frac{\\partial{J}}{\\partial{\\theta}_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial{J}}{\\partial{\\theta}_n} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\frac{1}{m}\n",
        "\\begin{bmatrix} \n",
        "\\sum^{m}_{i=1} [ -y^{(i)} \\log (h_{\\theta} (x^{(i)})) - (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)}))] \\\\\n",
        "\\sum^{m}_{i=1} [ -y^{(i)} \\log (h_{\\theta} (x^{(i)})) - (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)}))] \\\\ \n",
        "\\sum^{m}_{i=1} [ -y^{(i)} \\log (h_{\\theta} (x^{(i)})) - (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)}))] \\\\ \n",
        "\\vdots \\\\\n",
        "\\sum^{m}_{i=1} [ -y^{(i)} \\log (h_{\\theta} (x^{(i)})) - (1-y^{(i)}) \\log (1-h_{\\theta}(x^{(i)}))] \\\\ \n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{J}(\\theta)}{\\partial{\\theta}_0} =\n",
        "\\frac{1}{m}\n",
        "\\sum\\limits_{i=1}^m (h_0(x^{(i)})-y^{(i)})x^{(i)}_j \n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial{J}(\\theta)}{\\partial{\\theta}_0} = \\left (\n",
        "\\frac{1}{m}\n",
        "\\sum\\limits_{i=1}^m (h_0(x^{(i)})-y^{(i)})x^{(i)}_j \\right) +\n",
        "\\frac{\\lambda}{m} \\theta_j\n",
        "$$"
      ]
    }
  ]
}

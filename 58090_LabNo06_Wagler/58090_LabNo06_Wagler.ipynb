{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "58090_LabNo06_Wagler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikkuchan/CpE-AIML/blob/main/58090_LabNo06_Wagler/58090_LabNo06_Wagler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPzQ-B7jlCr"
      },
      "source": [
        "# Topic 05.2: Perceptrons, Gradient Descent, and Backpropagation\n",
        "$_{\\text{©D.J. Lopez | 2021 | Fundamentals of Machine Learning}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PM0SLUkk4q"
      },
      "source": [
        "## Laboratory Activity\n",
        "1. For the laboratory activity, obtain a dataset of your liking from a data source. Explain the purpose of the dataset and mention any publication if it is obtained from the source. Provide a needs statement and significance for the dataset.\n",
        "\n",
        "2. Identify an algorithm or method in performing a single or multiple variable classification using the Perceptron alogrithm. \n",
        "\n",
        "3. You must re-create your Perceptron algorithm with Gradient Descent and Backpropagation using your own code in a separate Google Colab. However, you are required to observe the following:\n",
        "\n",
        ">* Enforce object-oriented programming by implementing at least two of the pillars of OOP in the entirety of the solution.\n",
        "* Dedicated functions for training, predicting, and evaluating the solution.\n",
        "* A DataFrame of the metrics of the solution\n",
        "* A visualization of the solution’s results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTES: https://github.com/dyjdlopez/fund-of-aiml/blob/main/activities/05%20-%20Classification/fund_aiml_05v1_lec2_2021.ipynb \n"
      ],
      "metadata": {
        "id": "nXx_yBli7OhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os"
      ],
      "metadata": {
        "id": "aaBGIo4G7aaW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heartDisease = pd.read_csv(\"/content/heart_disease.csv\")\n",
        "y = heartDisease.target.values\n",
        "x_data = heartDisease.drop(['target'], axis = 1)"
      ],
      "metadata": {
        "id": "KNb2oS5L2YMR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)\n",
        "x_train = x_train.T\n",
        "y_train = y_train.T\n",
        "x_test = x_test.T\n",
        "y_test = y_test.T"
      ],
      "metadata": {
        "id": "wJAZqYkR6DaI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptrons:\n",
        "  def __init__(self, learningRate=1, iteration=100):\n",
        "    self.learningRate = learningRate\n",
        "    self.iteration = iteration\n",
        " \n",
        "  def initialize(dimension):  \n",
        "    weight = np.full((dimension,1),0.01)\n",
        "    bias = 0.0\n",
        "    return weight,bias\n",
        "  \n",
        "  def sigmoid(z):\n",
        "    y_head = 1/(1+ np.exp(-z))\n",
        "    return y_head\n",
        "\n",
        "  def forwardBackward(weight,bias,x_train,y_train):\n",
        "    # Forward Prp\n",
        "    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n",
        "    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n",
        "    cost = np.sum(loss) / x_train.shape[1]   \n",
        "    # Backward Prp\n",
        "    derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n",
        "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
        "    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n",
        "    return cost,gradients\n",
        "\n",
        "  def train(weight,bias,x_train,y_train) :\n",
        "    costList = []\n",
        "    index = []\n",
        "    for i in range(iteration):\n",
        "        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n",
        "        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n",
        "        bias = bias - learningRate * gradients[\"Derivative Bias\"]      \n",
        "        costList.append(cost)\n",
        "        index.append(i)\n",
        "    parameters = {\"weight\": weight,\"bias\": bias}\n",
        "    print(\"iteration:\",iteration)\n",
        "    print(\"cost:\",cost)\n",
        "    plt.plot(index,costList)\n",
        "    plt.xlabel(\"Number of Iteration\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "    return parameters, gradients\n",
        "  \n",
        "  def predict(weight,bias,x_test):\n",
        "    z = np.dot(weight.T,x_test) + bias\n",
        "    y_head = sigmoid(z)\n",
        "\n",
        "    y_prediction = np.zeros((1,x_test.shape[1]))\n",
        "    \n",
        "    for i in range(y_head.shape[1]):\n",
        "        if y_head[0,i] <= 0.5:\n",
        "            y_prediction[0,i] = 0\n",
        "        else:\n",
        "            y_prediction[0,i] = 1\n",
        "    return y_prediction\n",
        "\n",
        "  def logistic_regression(x_train,y_train,x_test,y_test):\n",
        "    dimension = x_train.shape[0]\n",
        "    weight,bias = initialize(dimension)\n",
        "    parameters, gradients = train(weight,bias,x_train,y_train)\n",
        "    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
        "    print(\"Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))"
      ],
      "metadata": {
        "id": "U9sN76Qr2m3J"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification = Perceptrons()"
      ],
      "metadata": {
        "id": "oGQzj63g5Sp3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GTmuTSOi7HuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}